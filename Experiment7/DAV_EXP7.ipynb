{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORpkyyDlLTB78dO+nHZRTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parth18062003/DAV/blob/main/Experiment7/DAV_EXP7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parth Kadam\n",
        "\n",
        "D11AD\n",
        "\n",
        "27"
      ],
      "metadata": {
        "id": "8YDbzmwTdg7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 7\n",
        "\n",
        "\n",
        "> **Aim:** Perform the steps involved in Text Analytics in Python & R\n",
        "\n",
        "> **Lab Objective:** To introduce the concept of text analytics and its applications.\n",
        "\n",
        ">**Lab Outcome:** Design Text Analytics Application on a given data set. (LO4)\n",
        "\n"
      ],
      "metadata": {
        "id": "IYtJt4Dje35_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task to be performed:**\n",
        "1. Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "2. Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "3. Perform the following experiments using Python & R\n",
        "* Tokenization (Sentence & Word)\n",
        "* Frequency Distribution\n",
        "* Remove stopwords & punctuations\n",
        "* Lexicon Normalization (Stemming, Lemmatization)\n",
        "* Part of Speech tagging\n",
        "* Named Entity Recognization\n",
        "* Scrape data from a website\n",
        "4. Prepare a document with the Aim, Tasks performed, Program, Output, and Conclusion."
      ],
      "metadata": {
        "id": "b-_GUx9EfeW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools & Libraries to be explored:**\n",
        "* Python Libraries: nltk, scattertext, SpaCy, TextBlob, sklearn, pandas, numpy\n",
        "* R Libraries: shiny, tm, quanteda"
      ],
      "metadata": {
        "id": "vhJ7y2eyf1FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory:**\n",
        "### Text analytics libraries in Python\n",
        "\n",
        "1. **NLTK:** NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "> **Features:**\n",
        "* Tokenization: NLTK provides functions for splitting text into words, sentences, or other linguistic units, making it easy to process text data.\n",
        "* Part-of-speech Tagging: NLTK includes pre-trained models for tagging words with their respective parts of speech (e.g., noun, verb, adjective), which is essential for many NLP tasks.\n",
        "* Named Entity Recognition (NER): NLTK offers tools for identifying and extracting named entities such as persons, organizations, locations, and dates from text documents.\n",
        "\n",
        "> **Applications:**\n",
        "* Text Classification: NLTK is widely used for building text classification models, including spam detection, sentiment analysis, topic classification, and document categorization.\n",
        "* Information Extraction: NLTK facilitates the extraction of structured information from unstructured text sources, including entity extraction, event extraction, and relationship extraction.\n",
        "* Language Understanding: NLTK enables the development of systems for understanding and interpreting human language, such as chatbots, virtual assistants, and question-answering systems.\n",
        "\n",
        "\n",
        "2. **SpaCy:** spaCy is an open-source library for advanced Natural Language Processing (NLP) in Python. It features pre-trained models for various languages, including English, German, French, and Spanish, and provides capabilities for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "\n",
        "> **Features:**\n",
        "* Dependency Parsing: spaCy supports dependency parsing, allowing for the analysis of syntactic relationships between words in a sentence and the construction of parse trees representing sentence structure.\n",
        "* Lemmatization: spaCy provides lemmatization functionality for reducing words to their base or canonical form, which helps in text normalization and word analysis.\n",
        "* Sentence Boundary Detection: spaCy includes algorithms for detecting sentence boundaries in text, enabling tasks such as sentence segmentation and sentence-level analysis.\n",
        "\n",
        "> **Applications:**\n",
        "* Information Retrieval: spaCy is employed in information retrieval systems for indexing, searching, and retrieving relevant documents or passages based on user queries or keywords.\n",
        "* Chatbots and Virtual Assistants: spaCy is used in the development of conversational agents, chatbots, and virtual assistants for natural language understanding, dialogue management, and response generation.\n",
        "* Text Mining and Analysis: spaCy is applied in text mining and analysis tasks, including document clustering, keyword extraction, trend detection, and content summarization.\n",
        "\n",
        "3. **TextBlob:** TextBlob is a simple and easy-to-use library for processing textual data in Python. It provides a simple API for common text processing tasks such as sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and classification.\n",
        "\n",
        "> **Features:**\n",
        "* Sentiment Analysis: TextBlob provides built-in sentiment analysis capabilities for assessing the sentiment or opinion expressed in a piece of text, including polarity (positive, negative, neutral) and subjectivity scores.\n",
        "* Language Detection: TextBlob can detect the language of a given text, enabling language identification and language-specific processing.\n",
        "* Translation: TextBlob offers support for language translation, allowing users to translate text between different languages using machine translation models.\n",
        "\n",
        "> **Applications:**\n",
        "* Text Classification: TextBlob is employed in text classification tasks, including document categorization, topic modeling, and sentiment-based classification in various domains.\n",
        "* Language Processing: TextBlob is utilized in language processing applications, such as language detection, translation, spell checking, and linguistic analysis.\n",
        "* Data Preprocessing: TextBlob is used for data preprocessing tasks in natural language processing pipelines, including text cleaning, normalization, and feature extraction.\n",
        "\n",
        "4. **ScatterText:** Scattertext is a free, opensouce python library for visualization of text data in different corpora in an interactive HTML scatterplot. It allows you to visualize how words are distributed in different documents or in different categories of documents.\n",
        "\n",
        "> **Features:**\n",
        "* Comparative Visualization: Scattertext specializes in visualizing the differences in word frequencies and associations between two or more corpora of text, allowing users to compare and contrast language use across different groups or categories.\n",
        "* Term Frequency Analysis: Scattertext provides tools for analyzing and visualizing the frequency of terms (words or phrases) in text data, highlighting terms that are more prevalent in one corpus compared to another.\n",
        "* Term Association Analysis: Scattertext identifies and visualizes associations between terms and categories, revealing which terms are strongly associated with specific groups or topics.\n",
        "\n",
        "> **Applications:**\n",
        "* Comparative Text Analysis: Scattertext is commonly used for comparative text analysis tasks, such as comparing language use between different groups (e.g., political parties, product reviews, social media posts) to identify distinctive terms and themes.\n",
        "*Sentiment Analysis: Scattertext can be applied to sentiment analysis tasks to examine differences in sentiment expression across different categories or topics, helping identify sentiment-related patterns and trends.\n",
        "* Authorship Attribution: Scattertext is used in authorship attribution studies to identify linguistic features that distinguish between different authors or writing styles, aiding in the analysis of authorship patterns and authorship identification.\n",
        "\n",
        "5. **Gensim:** Gensim is a Python library for topic modeling, document similarity analysis, and other natural language processing (NLP) tasks.\n",
        "\n",
        "> **Features:**\n",
        "* Topic Modeling: Gensim provides efficient implementations of popular topic modeling algorithms such as Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA). These algorithms allow users to discover latent topics in a collection of documents and analyze the distribution of topics across documents.\n",
        "* Word Embeddings: Gensim offers tools for training and using word embedding models such as Word2Vec, Doc2Vec, and FastText. These models learn dense vector representations of words and documents in a continuous vector space, capturing semantic relationships and similarities between words.\n",
        "* Document Similarity: Gensim allows users to compute similarities between documents based on their vector representations. This functionality is useful for tasks such as document clustering, information retrieval, and recommendation systems.\n",
        "\n",
        "> **Applications:**\n",
        "* Topic Modeling: Gensim is widely used for topic modeling tasks in various domains, including academic research, social media analysis, and content recommendation systems. It helps identify latent themes and topics in large collections of documents, enabling researchers and analysts to explore and understand complex textual data.\n",
        "* Document Clustering: Gensim's document similarity functionality is employed in document clustering applications to group similar documents together based on their content. It is used in information retrieval systems, content organization, and text classification tasks.\n",
        "* Natural Language Understanding: Gensim's word embedding models are used for natural language understanding tasks such as named entity recognition, sentiment analysis, and semantic similarity assessment. These models capture semantic relationships between words and phrases, facilitating more accurate and context-aware NLP applications.\n"
      ],
      "metadata": {
        "id": "OuDWZhM5f84f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGYixbqCrajg",
        "outputId": "08a4d753-0a0e-4907-b799-af1850724c09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = input(\"Enter text: \")\n",
        "\n",
        "print(\"\\n Sentence tokenization: \\n\" , sent_tokenize(text))\n",
        "print(\"\\n Word tokenization: \\n\" , word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCX2edoqGhO",
        "outputId": "98ebad28-bca5-4f6e-ced7-3fbcc9312de7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs.\n",
            "\n",
            " Sentence tokenization: \n",
            " ['Tokenization is the first step in text analytics.', 'The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization.', 'The token is a single entity that is building blocks for sentences or paragraphs.']\n",
            "\n",
            " Word tokenization: \n",
            " ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'text', 'paragraphs', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.', 'The', 'token', 'is', 'a', 'single', 'entity', 'that', 'is', 'building', 'blocks', 'for', 'sentences', 'or', 'paragraphs', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(word_tokenize(text))\n",
        "print(fdist.most_common(2))\n",
        "\n",
        "print(\"Frequency of each word: \\n\")\n",
        "for word, freq in fdist.items():\n",
        "    print(f'{word}: {freq}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdhYq_zisSzI",
        "outputId": "6420aeab-765a-486b-fb00-5c1c176e8699"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('is', 4), ('.', 3)]\n",
            "Frequency of each word: \n",
            "\n",
            "Tokenization: 2\n",
            "is: 4\n",
            "the: 1\n",
            "first: 1\n",
            "step: 1\n",
            "in: 1\n",
            "text: 2\n",
            "analytics: 1\n",
            ".: 3\n",
            "The: 2\n",
            "process: 1\n",
            "of: 1\n",
            "breaking: 1\n",
            "down: 1\n",
            "paragraphs: 2\n",
            "into: 1\n",
            "smaller: 1\n",
            "chunks: 1\n",
            "such: 1\n",
            "as: 1\n",
            "words: 1\n",
            "or: 2\n",
            "sentences: 2\n",
            "called: 1\n",
            "token: 1\n",
            "a: 1\n",
            "single: 1\n",
            "entity: 1\n",
            "that: 1\n",
            "building: 1\n",
            "blocks: 1\n",
            "for: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5X5NEH5tLwA",
        "outputId": "0a20a03e-0ac1-41e3-d20d-b207bc8cd9f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkvgDeYosAff",
        "outputId": "ac6b39b6-681b-4f67-8394-ecf7c7a9f0ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this', 'than', 'below', 'such', 'our', 'is', 'have', 'having', 'll', 'no', 'them', 'do', 'y', \"wouldn't\", 'o', 'not', 'because', \"wasn't\", 'me', 'there', 'aren', 'where', 'a', 'against', \"haven't\", 'these', 'what', \"you're\", \"doesn't\", 'ma', 'above', 'won', 'should', 'but', 'between', 'after', \"you'll\", 'which', 'only', 'both', 'further', 'herself', 'his', \"couldn't\", 'was', 't', 'under', 'through', 'wasn', 'why', 'that', \"you've\", 'yours', 'during', 'off', \"isn't\", 'too', 'shouldn', 'been', 'myself', \"shouldn't\", 'being', 'will', 'ours', 'does', 'doesn', 'as', 'any', \"shan't\", 'themselves', 'are', 'so', 'same', 'didn', 'did', \"should've\", 'if', 'more', 'you', 'they', 've', 'its', 'hasn', 'had', 'at', 'to', 'those', 'am', \"weren't\", \"don't\", 'your', 'again', 'the', 'hadn', 'ourselves', 'in', 'from', \"didn't\", 'we', 'before', \"aren't\", 'has', 'were', 'all', 'for', 'yourself', 'don', 's', 'each', 'hers', 'weren', 'own', 'most', 'yourselves', 'doing', 'out', \"you'd\", 'her', 'who', 'until', 'up', 'into', \"hadn't\", 'of', 'mightn', 'can', 'just', 'him', \"that'll\", \"mightn't\", 'few', 'over', 'd', 'wouldn', \"needn't\", 'nor', \"it's\", 'then', 'theirs', 'mustn', 'while', 'very', 'how', 'himself', 'he', 'itself', 'down', 'other', 'and', \"she's\", 'some', 'now', 'it', 'ain', 'isn', 'here', 'i', 'haven', 'couldn', 'or', 'with', 'on', 'once', 'my', 'needn', 'whom', 'be', \"mustn't\", 'an', 'she', 'their', 'about', 'm', 'when', 're', \"won't\", 'by', \"hasn't\", 'shan'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens=[]\n",
        "for w in word_tokenize(text):\n",
        "    if w not in stop_words:\n",
        "         filtered_tokens.append(w)\n",
        "\n",
        "print(\"Tokenized Words:\",word_tokenize(text))\n",
        "print(\"Filterd Tokens:\",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsxIHb55tZ1w",
        "outputId": "85992818-b69e-4a4c-9581-daff8b25d56c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'text', 'paragraphs', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.', 'The', 'token', 'is', 'a', 'single', 'entity', 'that', 'is', 'building', 'blocks', 'for', 'sentences', 'or', 'paragraphs', '.']\n",
            "Filterd Tokens: ['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'The', 'process', 'breaking', 'text', 'paragraphs', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', '.', 'The', 'token', 'single', 'entity', 'building', 'blocks', 'sentences', 'paragraphs', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuations=list(string.punctuation)\n",
        "\n",
        "filtered_tokens2=[]\n",
        "\n",
        "for i in filtered_tokens:\n",
        "    if i not in punctuations:\n",
        "        filtered_tokens2.append(i)\n",
        "\n",
        "print(\"Filterd Tokens After Removing Punctuations:\",filtered_tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXXULDp4tQPG",
        "outputId": "e219537b-6125-4571-df13-ce4e263703db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filterd Tokens After Removing Punctuations: ['Tokenization', 'first', 'step', 'text', 'analytics', 'The', 'process', 'breaking', 'text', 'paragraphs', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', 'The', 'token', 'single', 'entity', 'building', 'blocks', 'sentences', 'paragraphs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words=[]\n",
        "\n",
        "for w in filtered_tokens2:\n",
        "     stemmed_words.append(ps.stem(w))\n",
        "\n",
        "print(\"Filtered Tokens After Removing Punctuations:\",filtered_tokens2)\n",
        "print(\"Stemmed Tokens:\",stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLJMnQH3tuLW",
        "outputId": "3a5cfd7e-1d1a-42c0-a7e4-c329904eb75c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens After Removing Punctuations: ['Tokenization', 'first', 'step', 'text', 'analytics', 'The', 'process', 'breaking', 'text', 'paragraphs', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', 'The', 'token', 'single', 'entity', 'building', 'blocks', 'sentences', 'paragraphs']\n",
            "Stemmed Tokens: ['token', 'first', 'step', 'text', 'analyt', 'the', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentenc', 'call', 'token', 'the', 'token', 'singl', 'entiti', 'build', 'block', 'sentenc', 'paragraph']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Y5XfCsuKpg",
        "outputId": "00500196-dd7e-4e67-a35d-dd3883b0b8d3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs.\n",
            "Lemmatized Text: tokenization be the first step in text analytic . the process of break down text paragraph into small chunk such as word or sentence be call Tokenization . the token be a single entity that be build block for sentence or paragraph .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbOsejsfvVlW",
        "outputId": "aa2abac1-f82d-464a-b33b-fc0c238879c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "tokens=word_tokenize(text)\n",
        "pos_=pos_tag(tokens)\n",
        "\n",
        "\n",
        "print(\"Tokens:\",tokens)\n",
        "print(\"PoS tags:\",pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItTkpCDvHMb",
        "outputId": "26b6d5db-bd5e-463e-a02b-d566b272ed59"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'text', 'paragraphs', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.', 'The', 'token', 'is', 'a', 'single', 'entity', 'that', 'is', 'building', 'blocks', 'for', 'sentences', 'or', 'paragraphs', '.']\n",
            "PoS tags: [('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('The', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('text', 'JJ'), ('paragraphs', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('Tokenization', 'NNP'), ('.', '.'), ('The', 'DT'), ('token', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('single', 'JJ'), ('entity', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('building', 'VBG'), ('blocks', 'NNS'), ('for', 'IN'), ('sentences', 'NNS'), ('or', 'CC'), ('paragraphs', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bnpBZiwcDv",
        "outputId": "7b3f3407-df63-4211-a385-fb96febc86ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "for chunk in ne_chunk(nltk.pos_tag(word_tokenize(text))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md5O9RbLwILY",
        "outputId": "83986877-83a4-427d-c531-12defbe71551"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE Tokenization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text analytics libraries in R\n",
        "\n",
        "1. **tm:**\n",
        "The tm (Text Mining Infrastructure in R) package in R is a comprehensive framework for text mining tasks.\n",
        "\n",
        "> **Features:**\n",
        "* Text Preprocessing: tm provides functions for text cleaning, including removing punctuation, numbers, and stop words, stemming, and converting text to lower case.\n",
        "* Document-Term Matrix (DTM) Creation: It offers methods to create document-term matrices, a crucial data structure for text analysis, where rows represent documents and columns represent terms (words or n-grams).\n",
        "* Text Transformation: The package supports various transformations on text data, such as term frequency-inverse document frequency (TF-IDF) weighting and scaling.\n",
        "\n",
        "> **Applications:**\n",
        "* Document Classification: tm is used for categorizing documents into predefined classes or categories, such as spam detection, sentiment analysis, and topic classification.\n",
        "* Text Clustering: It facilitates grouping similar documents together based on their content, enabling tasks like document clustering and clustering-based topic modeling.\n",
        "* Text Retrieval: The package supports efficient retrieval of relevant documents based on search queries or similarity measures, enabling applications like information retrieval and recommendation systems.\n",
        "\n",
        "2. **quanteda:** The quanteda package in R is a comprehensive and powerful framework for quantitative text analysis.\n",
        "\n",
        "> **Features:**\n",
        "* Text Preprocessing: quanteda offers extensive text preprocessing capabilities, including tokenization, stemming, lemmatization, and stop word removal, to clean and prepare text data for analysis.\n",
        "* Document-Term Matrix (DTM) Creation: It provides efficient methods to create document-term matrices (DTMs) and document-feature matrices (DFMs), essential data structures for text analysis, allowing users to represent text corpora in a structured format.\n",
        "* Advanced Tokenization: The package supports advanced tokenization features, including n-grams, skip-grams, and user-defined tokenization rules, enabling fine-grained control over text processing.\n",
        "\n",
        "> **Applications:**\n",
        "* Social Science Research: quanteda is widely used in social science research for analyzing textual data from sources such as surveys, interviews, and social media to understand social phenomena, public opinion, and sentiment.\n",
        "* Political Analysis: It enables political scientists to analyze political texts, such as speeches, policy documents, and legislative texts, for studying political discourse, ideology, and policy preferences.\n",
        "* Market Research: The package is applied in market research and consumer analytics for analyzing customer feedback, product reviews, and social media data to identify trends, sentiments, and customer preferences.\n",
        "\n",
        "\n",
        " 3. **shiny:**  Shiny is an R package that enables the creation of interactive web applications directly from R.\n",
        "\n",
        "> **Features:**\n",
        "* Interactive Text Analysis: Shiny allows users to build interactive web applications for text analytics, enabling exploration and analysis of text data through interactive visualizations and tools.\n",
        "* Reactive Text Processing: With Shiny's reactive programming framework, changes in input text or parameters trigger updates to text processing and analysis, providing real-time feedback to users.\n",
        "* Customizable Text Widgets: Shiny offers customizable text input and output widgets, such as text areas, text inputs, and text outputs, allowing users to input, process, and display text data flexibly.\n",
        "\n",
        "> **Applications:**\n",
        "* Text Mining Dashboards: Shiny is used to create interactive dashboards for text mining and exploration, allowing users to interactively analyze and visualize text data patterns, trends, and insights.\n",
        "* Text Analytics Tools: It serves as a platform for building text analytics tools and applications for tasks such as sentiment analysis, topic modeling, text classification, and NER, providing users with customizable and user-friendly interfaces.\n",
        "* Text Data Visualization: Shiny applications enable the visualization of text data through interactive plots, word clouds, heatmaps, and network graphs, facilitating the exploration and interpretation of text data structures and relationships.\n",
        "\n",
        "4. **tidytext:** Tidy text format can be defined as a table with one-token-per-row. A token is any meaningful unit of text, such as a word, that we are interested in using for analysis.\n",
        "\n",
        "> **Features:**\n",
        "* Integration with Tidy Data Principles: tidytext follows the principles of tidy data, making it compatible with other tidyverse packages in R. This ensures consistency in data manipulation and analysis workflows.\n",
        "* Tokenization and Text Parsing: tidytext provides functions for tokenizing text data, splitting it into individual words or tokens, and parsing text into grammatical elements such as sentences, words, and n-grams.\n",
        "* Sentiment Analysis: The package includes functions for sentiment analysis, allowing users to analyze the sentiment of text data by associating each word with a sentiment score or sentiment category. This enables the identification of positive, negative, or neutral sentiment in text documents.\n",
        "\n",
        "\n",
        "> **Applications:**\n",
        "* Social Media Analysis: tidytext is used for analyzing text data from social media platforms, such as Twitter, Facebook, and Instagram. Researchers and marketers use the package to perform sentiment analysis, topic modeling, and trend analysis on social media content.\n",
        "* Customer Feedback Analysis: Businesses leverage tidytext for analyzing customer feedback data, including product reviews, surveys, and customer support tickets. By conducting sentiment analysis and word frequency analysis, companies can gain insights into customer sentiment, preferences, and pain points.\n",
        "* Text-Based Recommender Systems: tidytext is employed in building text-based recommender systems for recommending articles, products, or content based on textual similarity. By applying techniques such as TF-IDF and cosine similarity, developers can create personalized recommendations for users.\n",
        "\n",
        "\n",
        "5. **text2vec:** text2vec is an R package which provides an efficient framework with a concise API for text analysis and natural language processing (NLP).\n",
        "\n",
        "> **Features:**\n",
        "* Efficient Text Processing: text2vec is designed for efficient processing of large-scale text data, utilizing memory-mapped files and parallel processing techniques to handle big text corpora.\n",
        "* Modular Framework: The package offers a modular framework for text analysis, allowing users to build custom text processing pipelines by combining various vectorization, dimensionality reduction, and modeling techniques.\n",
        "* Vectorization Methods: text2vec provides several vectorization methods for converting text data into numerical representations, including Bag-of-Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), word embeddings (Word2Vec), and GloVe embeddings. These methods capture semantic and contextual information from text documents.\n",
        "\n",
        "> **Applications:**\n",
        "* Document Classification: text2vec is used for document classification tasks, such as sentiment analysis, topic categorization, and spam detection. By converting text documents into numerical vectors and training machine learning models, users can classify documents into predefined categories or labels.\n",
        "* Information Retrieval: The package is employed in information retrieval systems for searching and retrieving relevant documents based on user queries. By indexing and vectorizing text documents, information retrieval systems can quickly retrieve documents that match specific keywords or topics.\n",
        "* Text Clustering: text2vec facilitates text clustering tasks, where similar documents are grouped together based on their semantic similarity. By applying clustering algorithms to vectorized representations of text documents, users can discover latent clusters and organize large text corpora into meaningful groups."
      ],
      "metadata": {
        "id": "0aJmsJ5pl5aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9cCJyKnyNqI",
        "outputId": "923623d8-fe6e-4aa7-aca5-ebda315b0618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text analytics in R\n",
        "\n",
        "library(tokenizers)\n",
        "\n",
        "text <- readline(prompt = \"Enter text: \")\n",
        "\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "sentence_tokens <- unlist(tokenize_sentences(text))\n",
        "\n",
        "cat(\"\\nTokenized words:\\n\")\n",
        "print(word_tokens)\n",
        "\n",
        "cat(\"\\nTokenized sentences:\\n\")\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkuOuDriyB9R",
        "outputId": "efdf77f7-6687-4d54-f780-e5733f6510ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs.\n",
            "\n",
            "Tokenized words:\n",
            " [1] \"tokenization\" \"is\"           \"the\"          \"first\"        \"step\"        \n",
            " [6] \"in\"           \"text\"         \"analytics\"    \"the\"          \"process\"     \n",
            "[11] \"of\"           \"breaking\"     \"down\"         \"text\"         \"paragraphs\"  \n",
            "[16] \"into\"         \"smaller\"      \"chunks\"       \"such\"         \"as\"          \n",
            "[21] \"words\"        \"or\"           \"sentences\"    \"is\"           \"called\"      \n",
            "[26] \"tokenization\" \"the\"          \"token\"        \"is\"           \"a\"           \n",
            "[31] \"single\"       \"entity\"       \"that\"         \"is\"           \"building\"    \n",
            "[36] \"blocks\"       \"for\"          \"sentences\"    \"or\"           \"paragraphs\"  \n",
            "\n",
            "Tokenized sentences:\n",
            "[1] \"Tokenization is the first step in text analytics.\"                                                                  \n",
            "[2] \"The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization.\"\n",
            "[3] \"The token is a single entity that is building blocks for sentences or paragraphs.\"                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps involved in text analytics\n",
        "\n",
        "1. **Data gathering:**\n",
        "In this stage, you gather text data from internal or external sources.\n",
        "> Internal data: Internal data is text content that is internal to your business and is readily available—for example, emails, chats, invoices, and employee surveys.\n",
        "\n",
        " > External data:  You can find external data in sources such as social media posts, online reviews, news articles, and online forums. It is harder to acquire external data because it is beyond your control. You might need to use web scraping tools or integrate with third-party solutions to extract external data.\n",
        "\n",
        "2. **Data preparation:**\n",
        "Data preparation is an essential part of text analysis. It involves structuring raw text data in an acceptable format for analysis. The text analysis software automates the process and involves the following common natural language processing (NLP) methods.\n",
        "> Tokenization: Tokenization is segregating the raw text into multiple parts that make semantic sense. For example, the phrase text analytics benefits businesses tokenizes to the words text, analytics, benefits, and businesses.\n",
        "\n",
        " > Part-of-speech tagging: Part-of-speech tagging assigns grammatical tags to the tokenized text. For example, applying this step to the previously mentioned tokens results in text: Noun; analytics: Noun; benefits: Verb; businesses: Noun.\n",
        "\n",
        " >Parsing : Parsing establishes meaningful connections between the tokenized words with English grammar. It helps the text analysis software visualize the relationship between words.\n",
        "\n",
        " >Lemmatization: Lemmatization is a linguistic process that simplifies words into their dictionary form, or lemma. For example, the dictionary form of visualizing is visualize.\n",
        "\n",
        " >Stop words removal: Stop words are words that offer little or no semantic context to a sentence, such as and, or, and for. Depending on the use case, the software might remove them from the structured text.\n",
        "\n",
        "3. **Text analysis:**\n",
        "Text analysis is the core part of the process, in which text analysis software processes the text by using different methods.\n",
        "> Text classification: Classification is the process of assigning tags to the text data that are based on rules or machine learning-based systems.\n",
        "\n",
        " >Text extraction: Extraction involves identifying the presence of specific keywords in the text and associating them with tags. The software uses methods such as regular expressions and conditional random fields (CRFs) to do this.\n",
        "\n",
        "4. **Visualization:**\n",
        "Visualization is about turning the text analysis results into an easily understandable format. You will find text analytics results in graphs, charts, and tables. The visualized results help you identify patterns and trends and build action plans. For example, suppose you’re getting a spike in product returns, but you have trouble finding the causes. With visualization, you look for words such as defects, wrong size, or not a good fit in the feedback and tabulate them into a chart. Then you’ll know which is the major issue that takes top priority."
      ],
      "metadata": {
        "id": "ZUo2ZcfdkdOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Text Analytics\n",
        "* Helps in understanding emerging customer trends, product performance, and service quality.\n",
        "* Helps researchers to explore pre-existing literature and extracting what’s relevant to their study.\n",
        "* Text analytic techniques help search engines to improve their performance, thereby providing fast user experiences.\n",
        "* Helps in making more data-driven decisions\n",
        "* Refines user content recommendation systems by categorizing related content\n",
        "* Boost Efficiency of working with Unstructured data"
      ],
      "metadata": {
        "id": "bQqgY5Qrlt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Conclusion:\n",
        "* Identified the Text Analytics Libraries in Python and R\n",
        "* Performed simple experiments with these libraries in Python and R"
      ],
      "metadata": {
        "id": "j-rToYA0yifH"
      }
    }
  ]
}